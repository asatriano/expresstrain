{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"expresstrain_fashion_mnist_example.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMyJyf/Jq4sA3V1KG0SmoQb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2fOeISCBJcuT"},"source":["# %%\n","# Adapted from:\n","# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","\n","from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","import os\n","\n","# Clone ExpressTrain\n","os.system(\"rm -rf ./expresstrain\")\n","os.system(\"git clone https://github.com/asatriano/expresstrain\")\n","# Import ExpressTrain :)\n","import expresstrain as et\n","\n","# %%\n","# Define your model:\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x \n","    #logits (or compute logsoftmax, and use a NLLLoss as a \n","    # custom loss function in ExpressTrainer)\n","\n","# %%\n","# Set up some hyperparameters:\n","# input random seed integer (default: 42) (type=int):\n","random_seed=42\n","# input batch size to use at training (default: 32) (type=int):\n","batch_size=32\n","# input batch size multiplier for validation (default: 2) (type=int):\n","batch_size_multiplier=2\n","# input number of workers for dataloaders (default: 0) (type=int):\n","num_workers_dataloader=0\n","# input training learnign rate (default: 3e-4) (type=float):\n","learning_rate=1e-2\n","# input training epochs (default=10) (type=int):\n","epochs=30\n","# input saving path for loss and metrics (default: None) (type=str):\n","path_performance=None\n","# input saving path for loss, metric, and model params (default: None) (type=str):\n","path_perf_model=None\n","# input whether to use Automatic Mixed Precision (default: True) (type=bool):\n","use_fp16=False\n","# input how many batches between backward passes (default: 1) (type=bool):\n","backward_every=1\n","\n","# %%\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device used: {device}\")\n","  \n","# Define your transforms:\n","transform=transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","\n","# Import your datasets\n","dataset1 = datasets.FashionMNIST('./data', train=True, download=True,\n","                    transform=transform)\n","dataset2 = datasets.FashionMNIST('./data', train=False,\n","                    transform=transform)\n","\n","# Define Dataloaders:\n","assert(batch_size>backward_every)\n","train_kwargs = {'batch_size': batch_size//backward_every,\n","                'shuffle': True}\n","valid_kwargs = {'batch_size': batch_size*batch_size_multiplier//backward_every,\n","                'shuffle': False}\n","workers_kwargs = {'num_workers': num_workers_dataloader}\n","\n","train_kwargs.update(workers_kwargs)\n","valid_kwargs.update(workers_kwargs)\n","train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n","valid_loader = torch.utils.data.DataLoader(dataset2, **valid_kwargs)\n","\n","# Instance your favourite model and optimizer\n","model = Net().to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Define your favourite metric\n","def accuracy(preds, targets):\n","    assert(len(preds)==len(targets))\n","    correct=torch.sum(preds == targets)\n","    return correct/len(targets)\n","\n","metric_used=accuracy\n","\n","# Subclass Express Train:\n","class CustomExpressTrain(et.ExpressTrain):\n","    def __init__(self, **kwargs):\n","        super(CustomExpressTrain, self).__init__()\n","        self.initialize_all(kwargs)\n","\n","    def on_train_epoch_start(self):\n","        print(f\"Message before epoch {self.epoch+1} - Today is a great day :)\")\n","\n","# Instance your Custom Express Train trainer\n","trainer_kwargs={'train_loader': train_loader,\n","                'valid_loader': valid_loader,\n","                'model': model,\n","                'num_classes': 10,\n","                'device': device,\n","                'learning_rate': learning_rate,\n","                'optimizer': optimizer,\n","                'metric_used': metric_used,\n","                'path_performance': path_performance,\n","                'path_performance_and_model': path_perf_model,\n","                'backward_every': backward_every}\n","if use_fp16==True:\n","    print(\"Using Automatic Mixed Precision\")\n","    trainer_kwargs.update({'fp16': use_fp16})\n","\n","trainer=CustomExpressTrain(**trainer_kwargs)\n","\n","trainer.fit(epochs)\n"],"execution_count":null,"outputs":[]}]}